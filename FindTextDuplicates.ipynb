{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0afbb-1543-4197-a65c-3f346d0e4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dawn Schmidt\n",
    "#Machine Learning 2\n",
    "#Assignment 3\n",
    "#August 10, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c748423-0927-4199-bfb6-7cf2b4f3d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#filter specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.tokenization_utils_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cecaa7-a1f5-43cb-9d1c-2d2c81cc1c38",
   "metadata": {},
   "source": [
    "# Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ce766a5b-7373-41c2-be46-75e5f212f024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the data file and remove <start> and <end> tokens; replace with ''\n",
    "data=json.load(open('data_assignment3.json'))\n",
    "with open('data_assignment3.json') as f:\n",
    "    data = json.load(f)\n",
    "cleaned_data = [sentence.replace('<start>', '').replace('<end>', '').strip() for sentence in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "002b6096-72d3-47b7-89ef-9bf641784328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man rows a boat against a night sky .',\n",
       " 'Two dogs running through a field .',\n",
       " 'A small boy wearing a red helmet rides his bicycle down a patterned path .',\n",
       " 'A black dog standing in a shallow area of water on a rocky beach .',\n",
       " 'Two white dogs are running together .']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show top 5 rows of data of cleaned data\n",
    "cleaned_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc005a-2aea-4406-8e47-9bb57665a34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b34b6cda-a076-4a66-9ecb-4d706e99c9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man rows boat night sky', 'two dogs running field', 'small boy wearing red helmet rides bicycle patterned path', 'black dog standing shallow area water rocky beach', 'two white dogs running together']\n"
     ]
    }
   ],
   "source": [
    "#initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#function for cleaning text\n",
    "def clean_text(text):\n",
    "    #normalize case to lower case\n",
    "    text = text.lower()\n",
    "    #remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#apply cleaning to all sentences\n",
    "cleaned_data = [clean_text(sentence) for sentence in cleaned_data]\n",
    "\n",
    "#print the first 5 cleaned sentences\n",
    "print(cleaned_data[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc37f9-fdcb-4c4c-b1ef-90fc43f09f72",
   "metadata": {},
   "source": [
    "# Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "758ced29-de21-4038-afef-cddf4bdee2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 384)\n"
     ]
    }
   ],
   "source": [
    "#load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#generate embeddings for each sentence\n",
    "embeddings = model.encode(cleaned_data)\n",
    "\n",
    "#print the shape of embeddings to verify\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bead9b1-cc9b-486d-9825-9b9ada93c230",
   "metadata": {},
   "source": [
    "# Group Sentences by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8feebd62-d65f-4c7d-9be5-78ca53075be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping complete\n"
     ]
    }
   ],
   "source": [
    "#calculate cosine similarity between all pairs\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "#create groups ensuring each group has exactly 5 sentences\n",
    "used_sentences = set()\n",
    "groups = []\n",
    "\n",
    "for i in range(len(cleaned_data)):\n",
    "    if i in used_sentences:\n",
    "        continue\n",
    "    \n",
    "    #get the indices of the most similar sentences\n",
    "    similar_indices = np.argsort(-similarity_matrix[i])[1:6]\n",
    "    \n",
    "    #ensure we don't use already used sentences and that we form a group of 5\n",
    "    group = [i]\n",
    "    for idx in similar_indices:\n",
    "        if idx not in used_sentences and len(group) < 5:\n",
    "            group.append(idx)\n",
    "    \n",
    "    if len(group) == 5:\n",
    "        groups.append(group)\n",
    "        used_sentences.update(group)\n",
    "        \n",
    "print('Grouping complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0db8aa-4a13-405d-b19b-148f3bf29283",
   "metadata": {},
   "source": [
    "# Export results .txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e87c6b0-a43a-41a3-b0de-fcdad879beae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to grouped_sentences_output.txt\n"
     ]
    }
   ],
   "source": [
    "#write the grouped sentences to a text file\n",
    "with open('grouped_sentences_output.txt', 'w') as f:\n",
    "    for group in groups:\n",
    "        for idx in group:\n",
    "            f.write(cleaned_data[idx] + '\\n')\n",
    "        f.write('---\\n')\n",
    "\n",
    "print(\"Output written to grouped_sentences_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2085a-5d07-4e42-bc9e-56be723d76a2",
   "metadata": {},
   "source": [
    "# Check submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e8084fe3-285f-4002-872e-dfeadea1f852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m-----------------PASS-----------------\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Define the filename directly, since sys.argv doesn't apply in a notebook environment\n",
    "file_name = 'TEST.txt'\n",
    "\n",
    "def check_if_my_submission_is_correct(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            count = 0\n",
    "            line_set = set()\n",
    "            for line in f.read().splitlines():\n",
    "                break_line = ''.join(set(line)) == '-'\n",
    "                if break_line:\n",
    "                    if count != 5:\n",
    "                        msg = f'The number of lines after last break line is {count}, but it should be 5'\n",
    "                        msg += '\\n' + 'There needs to be a break line after every 5 entries, and the break line should consist only of \"-\"'\n",
    "                        print('\\033[31m' + '-----------------FAIL-----------------' + '\\033[m')\n",
    "                        print(msg)\n",
    "                        break\n",
    "                    count = 0\n",
    "                else:\n",
    "                    if line in line_set:\n",
    "                        msg = f'Duplicate entry: {line}'\n",
    "                        msg += '\\n' + 'There should be no duplicate entry'\n",
    "                        print('\\033[31m' + '-----------------FAIL-----------------' + '\\033[m')\n",
    "                        print(msg)\n",
    "                        break\n",
    "                    line_set.add(line)\n",
    "                    count += 1\n",
    "            else:\n",
    "                print('\\033[32m' + '-----------------PASS-----------------' + '\\033[m')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_name}. Please check the file path and try again.\")\n",
    "\n",
    "# Check the submission file\n",
    "check_if_my_submission_is_correct(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59867d9f-e24d-4580-9d5a-51a35fa14922",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TEST###\n",
    "\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers.tokenization_utils_base\")\n",
    "\n",
    "### Section 1: Load and Clean the Data ###\n",
    "\n",
    "# Load the data file\n",
    "with open('data_assignment3.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Remove <start> and <end> tokens from original data\n",
    "original_sentences = [sentence.replace('<start>', '').replace('<end>', '').strip() for sentence in data]\n",
    "\n",
    "# Remove exact duplicates\n",
    "unique_original_sentences = list(set(original_sentences))\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Normalize case to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Create a mapping of cleaned sentences to original sentences\n",
    "cleaned_to_original = {clean_text(sentence): sentence for sentence in unique_original_sentences}\n",
    "\n",
    "# Create a list of cleaned sentences\n",
    "cleaned_data = list(cleaned_to_original.keys())\n",
    "\n",
    "# Optional: Check the first 5 cleaned sentences\n",
    "print(\"Sample of cleaned data:\", cleaned_data[:5])\n",
    "\n",
    "### Section 2: Generate Sentence Embeddings ###\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each cleaned sentence\n",
    "embeddings = model.encode(cleaned_data)\n",
    "\n",
    "# Optional: Print the shape of embeddings to verify\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "### Section 3: Group Sentences by Similarity, Avoiding Duplicates ###\n",
    "\n",
    "# Calculate cosine similarity between all pairs of embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create groups ensuring each group has exactly 5 sentences without duplicates\n",
    "used_sentences = set()\n",
    "groups = []\n",
    "\n",
    "for i in range(len(cleaned_data)):\n",
    "    if i in used_sentences:\n",
    "        continue\n",
    "    \n",
    "    # Get the indices of the most similar sentences\n",
    "    similar_indices = np.argsort(-similarity_matrix[i])[1:]\n",
    "    \n",
    "    # Form a group of 5 sentences, avoiding duplicates\n",
    "    group = [i]\n",
    "    for idx in similar_indices:\n",
    "        if idx not in used_sentences and len(group) < 5:\n",
    "            group.append(idx)\n",
    "    \n",
    "    if len(group) == 5:\n",
    "        groups.append(group)\n",
    "        used_sentences.update(group)\n",
    "\n",
    "### Section 4: Print Top 10 Lines Before Exporting ###\n",
    "\n",
    "# Prepare the first 10 lines for display\n",
    "output_lines = []\n",
    "for group in groups[:2]:  # 2 groups of 5 sentences = 10 lines\n",
    "    for idx in group:\n",
    "        output_lines.append(cleaned_to_original[cleaned_data[idx]])\n",
    "    output_lines.append('---')\n",
    "\n",
    "# Print the top 10 lines\n",
    "print(\"\\n\".join(output_lines))\n",
    "\n",
    "### Section 5: Export the Groups to a TXT File ###\n",
    "\n",
    "# Write the grouped original sentences to a text file\n",
    "with open('grouped_sentences_output.txt', 'w') as f:\n",
    "    for group in groups:\n",
    "        for idx in group:\n",
    "            f.write(cleaned_to_original[cleaned_data[idx]] + '\\n')\n",
    "        f.write('---\\n')\n",
    "\n",
    "print(\"Output written to grouped_sentences_output.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
